{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "### COSC 3337 Dr.Rizk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Kim Nguyen> \n",
      "last updated: 2020-06-18 \n",
      "\n",
      "CPython 3.7.6\n",
      "IPython 7.12.0\n",
      "\n",
      "numpy 1.18.1\n",
      "scipy 1.4.1\n",
      "matplotlib 3.1.3\n",
      "sklearn 0.22.1\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -d -u -a '<Kim Nguyen>' -v -p numpy,scipy,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Implementing an ID3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1   Splitting a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(array):\n",
    "    # your code to generate dictionary\n",
    "    res = {classi: np.where(array == classi)[0] for classi in np.unique(array)}\n",
    "    return res # return the dictionary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0]), 1: array([1]), 2: array([2])}\n",
      "{0: array([1, 3, 4, 6]), 1: array([0, 2, 5])}\n",
      "{0: array([1, 4]), 1: array([0, 5, 6]), 2: array([3]), 3: array([2])}\n"
     ]
    }
   ],
   "source": [
    "# Double check solution\n",
    "print(split(np.array([0, 1, 2])))\n",
    "print(split(np.array([1, 0, 1, 0, 0, 1, 0])))\n",
    "print(split(np.array([1, 0, 3, 2, 0, 1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2   Implement Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(array):\n",
    "    # your code\n",
    "    N = len(array)\n",
    "    counts = np.bincount(array)\n",
    "    P = counts[np.nonzero(counts)]/N # avoid log(0)\n",
    "    H = -np.dot(P, np.log2(P))\n",
    "    return H # return a scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "-0.0\n",
      "0.4395\n",
      "-0.0\n",
      "1.6577\n"
     ]
    }
   ],
   "source": [
    "# Double check solution\n",
    "print(round(entropy(np.array([0, 1, 0, 1, 1, 0])), 4))\n",
    "print(round(entropy(np.array([1, 2])), 4))\n",
    "print(round(entropy(np.array([1, 1])), 4))\n",
    "print(round(entropy(np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), 4))\n",
    "print(round(entropy(np.array([0, 0, 0])), 4))\n",
    "print(round(entropy(np.array([1, 1, 1, 0, 1, 4, 4, 2, 1])), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3   Implement Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(x_array, y_array):\n",
    "    parent_entropy = entropy(x_array)\n",
    "\n",
    "    split_dict = split(y_array)\n",
    "\n",
    "    for val in split_dict.values():\n",
    "        freq = val.size/x_array.size\n",
    "        child_entropy = entropy([x_array[i] for i in val])\n",
    "        parent_entropy -= child_entropy*freq\n",
    "\n",
    "    return parent_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4591\n",
      "0.2516\n"
     ]
    }
   ],
   "source": [
    "# Double check solution\n",
    "\n",
    "x = np.array([0, 1, 0, 1, 0, 1])\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "print(round(information_gain(x, y), 4))\n",
    "\n",
    "x = np.array([0, 0, 1, 1, 2, 2])\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "print(round(information_gain(x, y), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Decision Tree Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(X, y):\n",
    "# Return array if node is empty or pure (1 example in leaf node)\n",
    "    \n",
    "    if y.shape[0] == 1 or y.shape[0] == 0:\n",
    "        return y\n",
    "\n",
    "    # Compute information gain for each feature\n",
    "    gains = [information_gain(x_attribute,y) for x_attribute in X.T] # YOUR CODE\n",
    "    \n",
    "    # Early stopping if there is no information gain\n",
    "    if all(eachGain <= 1e-05 for eachGain in gains):\n",
    "        return y # YOUR CODE\n",
    "\n",
    "    # Else, get best feature\n",
    "    best_feature = np.argmax(gains)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Use the `split` function to split on the best feature\n",
    "    subset_dict = split(X[:, best_feature])\n",
    "    \n",
    "    # Note that each entry in the dictionary returned by\n",
    "    # split is an attribute_value:array_indices pair.\n",
    "    # here, we are going to iterate over these key-value\n",
    "    # pairs and select the respective examples for the\n",
    "    # new child nodes\n",
    "    \n",
    "    for feature_value, train_example_indices in subset_dict.items():\n",
    "        child_y_subset = y.take(train_example_indices, axis = 0) # YOUR CODE\n",
    "        child_x_subset = X.take(train_example_indices, axis = 0) # YOUR CODE\n",
    "\n",
    "        # Next, we are using \"recursion,\" that is, calling the same\n",
    "        # tree_split function on the child subset(s)\n",
    "        \n",
    "        results[\"X_%d = %d\" % (best_feature, feature_value)] = \\\n",
    "            make_tree(child_x_subset, child_y_subset)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [2 1]]\n",
      "\n",
      "Labels:\n",
      " [0 1 0 1 1 1]\n",
      "\n",
      "Decision tree:\n",
      " {'X_1 = 0': {'X_0 = 0': array([0]), 'X_0 = 1': array([0]), 'X_0 = 2': array([1])}, 'X_1 = 1': array([1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# Double check solution\n",
    "x1 = np.array([0, 0, 1, 1, 2, 2])\n",
    "x2 = np.array([0, 1, 0, 1, 0, 1])\n",
    "X = np.array([x1, x2]).T\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "\n",
    "print('Inputs:\\n', X)\n",
    "print('\\nLabels:\\n', y)\n",
    "\n",
    "print('\\nDecision tree:\\n', make_tree(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Building a Decision Tree API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3DecisionTreeClassifer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.splits_ = make_tree(X,y) #YOUR CODE\n",
    "        \n",
    "    def _majority_vote(self, label_array):\n",
    "        return np.argmax(np.bincount(label_array)) #YOUR CODE\n",
    "    \n",
    "    def _traverse(self, x, d):\n",
    "        if isinstance(d, np.ndarray):\n",
    "            return d\n",
    "        for key in d:\n",
    "            name, value = key.split(' = ')\n",
    "            feature_idx = int(name.split('_')[-1])\n",
    "            value = int(value)\n",
    "            if x[feature_idx] == value:\n",
    "                return self._traverse(x, d[key])\n",
    "            \n",
    "    def predict(self, x): \n",
    "        \n",
    "        label_array = self._traverse(x,self.splits_) #YOUR CODE to get class labels from the target node\n",
    "        return self._majority_vote(label_array) #YOUR CODE to predict the class label via majority voting from label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Double check solution\n",
    "\n",
    "tree = ID3DecisionTreeClassifer()\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(tree.predict(np.array([0, 0])))\n",
    "print(tree.predict(np.array([0, 1])))\n",
    "print(tree.predict(np.array([1, 0])))\n",
    "print(tree.predict(np.array([1, 0])))\n",
    "print(tree.predict(np.array([1, 1])))\n",
    "print(tree.predict(np.array([2, 0])))\n",
    "print(tree.predict(np.array([2, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bootrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 150\n",
      "Number of features: 4\n",
      "Unique class labels: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "from mlxtend.data import iris_data\n",
    "X, y = iris_data()\n",
    "\n",
    "print('Number of examples:', X.shape[0])\n",
    "print('Number of features:', X.shape[1])\n",
    "print('Unique class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 105\n",
      "Number of test examples: 45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split #YOUR CODE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 45, random_state=123, stratify=y) # YOUR CODE \n",
    "\n",
    "print('Number of training examples:', X_train.shape[0])\n",
    "print('Number of test examples:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 105\n",
      "Number of test examples: 45\n"
     ]
    }
   ],
   "source": [
    "# Double check solution\n",
    "print('Number of training examples:', X_train.shape[0])\n",
    "print('Number of test examples:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bootstrap_sample(rng, X, y):\n",
    "    sample_indices = np.arange(X_train.shape[0]) # YOUR CODE\n",
    "    bootstrap_indices = rng.choice(sample_indices,size=sample_indices.shape[0]) # YOUR CODE\n",
    "    return X[bootstrap_indices], y[bootstrap_indices] # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training inputs from bootstrap round: 105\n",
      "Number of training labels from bootstrap round: 105\n",
      "Labels:\n",
      " [0 0 1 0 0 1 2 0 2 1 0 0 2 1 1 1 1 2 1 1 2 0 2 1 2 1 1 1 0 1 0 0 1 2 0 0 0\n",
      " 0 2 1 1 2 1 2 1 1 2 1 2 0 1 1 2 2 1 0 1 0 2 2 0 1 0 2 0 0 0 0 1 2 0 0 1 0\n",
      " 1 1 0 1 1 2 2 0 2 0 2 0 1 1 2 2 0 2 2 2 0 1 0 1 2 2 2 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Double check solution\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "\n",
    "print('Number of training inputs from bootstrap round:', X_boot.shape[0])\n",
    "print('Number of training labels from bootstrap round:', y_boot.shape[0])\n",
    "print('Labels:\\n', y_boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bagging classifier from decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class BaggingClassifier(object):\n",
    "    def __init__(self, num_trees=10, random_state=123):\n",
    "        self.num_trees = num_trees\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.trees_ = [DecisionTreeClassifier(random_state=self.rng) for i in range(self.num_trees)]\n",
    "        for i in range(self.num_trees):\n",
    "            X_boot, y_boot = draw_bootstrap_sample(self.rng, X, y) #YOUR CODE\n",
    "            # YOUR CODE to\n",
    "            # fit the trees in self.trees_ on the bootstrap samples\n",
    "            self.trees_[i].fit(X_boot,y_boot) #YOUR CODE\n",
    "            \n",
    "    def predict(self, X):\n",
    "        ary = np.zeros((X.shape[0], len(self.trees_)), dtype=np.int)\n",
    "        for i in range(len(self.trees_)):\n",
    "            ary[:, i] = self.trees_[i].predict(X)\n",
    "            \n",
    "        maj = np.apply_along_axis(lambda x: \n",
    "                                      np.argmax(np.bincount(x)),\n",
    "                                      axis=1,\n",
    "                                      arr=ary)\n",
    "        return maj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Tree Accuracies:\n",
      "88.9%\n",
      "93.3%\n",
      "97.8%\n",
      "93.3%\n",
      "93.3%\n",
      "93.3%\n",
      "91.1%\n",
      "97.8%\n",
      "97.8%\n",
      "97.8%\n",
      "\n",
      "Bagging Test Accuracy: 97.8%\n"
     ]
    }
   ],
   "source": [
    "# Double check solution\n",
    "\n",
    "model = BaggingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print('Individual Tree Accuracies:')\n",
    "\n",
    "for tree in model.trees_:\n",
    "    predictions = tree.predict(X_test)\n",
    "    print('%.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))\n",
    "    \n",
    "print('\\nBagging Test Accuracy: %.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Bias-Variance Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bias_Variance decomposition of the 0-1 Loss for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "num_bootstrap = 200\n",
    "\n",
    "all_pred = np.zeros((num_bootstrap, y_test.shape[0]), dtype=np.int)\n",
    "\n",
    "for i in range(num_bootstrap):\n",
    "    X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "    pred = DecisionTreeClassifier(random_state=66).fit(X_boot, y_boot).predict(X_test)\n",
    "    all_pred[i] = pred\n",
    "    \n",
    "main_predictions = np.apply_along_axis(lambda x: \n",
    "                                       np.argmax(np.bincount(x)),\n",
    "                                       axis=0,\n",
    "                                       arr=all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Bias:  0.0222\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "avg_bias = round(np.sum(main_predictions != y_test) / y_test.size,4)\n",
    "print(\"Average Bias: \", avg_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average variance:  0.0346\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "# you probably need multiple\n",
    "# lines of code and a for-loop\n",
    "\n",
    "var = np.zeros(pred.shape)\n",
    "\n",
    "for pred in all_pred:\n",
    "    var += (pred != main_predictions).astype(np.int)\n",
    "var /= num_bootstrap\n",
    "\n",
    "avg_var = round(var.sum()/y_test.shape[0],4)\n",
    "print(\"Average variance: \", avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bias_Variance decomposition of the 0-1 Loss for Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION\n",
    "# Many lines of code (which you may copy and modify from 3.1)\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "num_bootstrap = 200\n",
    "all_pred = np.zeros((num_bootstrap, y_test.shape[0]), dtype=np.int)\n",
    "\n",
    "for i in range(num_bootstrap):\n",
    "    X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "    model = BaggingClassifier()\n",
    "    model.fit(X_boot, y_boot)\n",
    "    pred= model.predict(X_test)\n",
    "    all_pred[i] = pred\n",
    "    \n",
    "main_predictions = np.apply_along_axis(lambda x: \n",
    "                                       np.argmax(np.bincount(x)),\n",
    "                                       axis=0,\n",
    "                                       arr=all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Bias:  0.0222\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "avg_bias = round(np.sum(main_predictions != y_test) / y_test.size,4)\n",
    "print(\"Average Bias: \", avg_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average variance:  0.0281\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "# you probably need multiple\n",
    "# lines of code and a for-loop\n",
    "\n",
    "var = np.zeros(pred.shape)\n",
    "\n",
    "for pred in all_pred:\n",
    "    var += (pred != main_predictions).astype(np.int)\n",
    "var /= num_bootstrap\n",
    "\n",
    "avg_var = round(var.sum()/y_test.shape[0],4)\n",
    "print(\"Average variance: \", avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to the Bias_Variance decomposition of the 0-1 Loss for Decision Trees in 3.1\n",
    "    ### => Average Bias is the same (0.0222)\n",
    "    ### => Average variance is lower than in 3.1 (0.0281 < 0.0346)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Bias-Variance decomposition of the 0-1 Loss for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION\n",
    "# Many lines of code (which you may copy and modify from 3.1)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "num_bootstrap = 200\n",
    "all_pred = np.zeros((num_bootstrap, y_test.shape[0]), dtype=np.int)\n",
    "\n",
    "for i in range(num_bootstrap):\n",
    "    X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "    pred = AdaBoostClassifier(random_state=66).fit(X_boot, y_boot).predict(X_test)\n",
    "    all_pred[i] = pred\n",
    "    \n",
    "main_predictions = np.apply_along_axis(lambda x: \n",
    "                                       np.argmax(np.bincount(x)),\n",
    "                                       axis=0,\n",
    "                                       arr=all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Bias:  0.0444\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "avg_bias = round(np.sum(main_predictions != y_test) / y_test.size,4)\n",
    "print(\"Average Bias: \", avg_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average variance:  0.0328\n"
     ]
    }
   ],
   "source": [
    "var = np.zeros(pred.shape)\n",
    "\n",
    "for pred in all_pred:\n",
    "    # (np.array(something) == np.array(something)).astype(np.int), the condition is only for numpy array \n",
    "    var += (pred != main_predictions).astype(np.int)\n",
    "var /= num_bootstrap\n",
    "\n",
    "avg_var = round(var.sum()/y_test.shape[0],4)\n",
    "\n",
    "print(\"Average variance: \", avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to the Bias_Variance decomposition of the 0-1 Loss for Decision Trees in 3.1\n",
    "    ### => Average Bias is higher than in 3.1 (0.0444 > 0.0222)\n",
    "    ### => Average variance is a litlle lower than in 3.1 (0.0328 > 0.0346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
